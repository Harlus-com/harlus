{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. if no bounding box is found return course bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/ronald/Documents/code/harlus/ml/tools/harlus_chat\")\n",
    "from src.harlus_chat.build_graph import ChatAgentGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating nodes for /Users/ronald/Documents/code/harlus/ml/tools/harlus_chat/attributes/test1.pdf ...\n",
      " - parsing PDF to JSON...\n",
      "Started parsing the file under job_id 356e14eb-68da-4c78-9775-1f8e49d9a3f8\n",
      " - creating nodes from JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 58734.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding node relationships...\n",
      " - splitting tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  > splitting tables:   0%|          | 0/41 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "1it [00:00, 345.67it/s]\n",
      "1it [00:00, 457.05it/s]\n",
      "1it [00:00, 348.51it/s]\n",
      "1it [00:00, 326.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:00, 22192.08it/s]\n",
      "1it [00:00, 1076.57it/s]\n",
      "  > splitting tables: 100%|██████████| 41/41 [00:01<00:00, 27.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding metadata to nodes...\n",
      "  > extracting metadata for SummaryExtractor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:02<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > extracting metadata for KeywordExtractor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:03<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - splitting text nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  > splitting text nodes: 100%|██████████| 168/168 [00:00<00:00, 4828.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding file path to nodes...\n",
      "getting single doc query engine from nodes\n",
      " - building vector index ...\n",
      " - building vector retriever ...\n",
      " - building keyword index ...\n",
      " - building summary index ...\n",
      " - building keyword retriever ...\n",
      " - building mix keyword vector retriever ...\n",
      " - building recursive keyword vector retriever ...\n",
      " - building mix keyword vector retriever query engine ...\n",
      " - building summary index query engine...\n",
      " - extracting metadata from query engines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata: 100%|██████████| 7/7 [01:05<00:00,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - building mix retriever query engine tool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from doc_search import DocToolLoader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "toolloader = await DocToolLoader().load(\n",
    "    file_path=\"/Users/ronald/Documents/code/harlus/ml/tools/harlus_chat/attributes/test1.pdf\",\n",
    "    file_name= \"test1\"\n",
    ")\n",
    "tool = toolloader.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[harlus_chat] Streaming answer...\n",
      "  Reading  [ Form   10 -K  for  the  Fiscal  Year  End ed  September   28 ,   202 4 ]  on  [ Apple  Inc .]  to  verify  if  [ Apple  has  a  business  segment  in  the  Americas ] ...                   Yes ,  Apple  has  a  business  segment  in  the  Americas ,  which  includes  both  North  and  South  America . [harlus_chat] Streaming source annotations...\n",
      "[harlus_chat] Sent 2 source annotations\n",
      " 4f3ce7cb-a72d-491f-989a-2034dd0e14ce, file_idError processing chunk: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "gp = ChatAgentGraph([tool.to_langchain_tool()])\n",
    "g =  gp.build()\n",
    "async for chunk in gp.stream(\"Does apple have a business segment in the americas?\"):\n",
    "    try:\n",
    "        chunk_data = chunk.split('\\n')[0].split(': ', 1)[1]\n",
    "        chunk_data = chunk_data.replace('data: ', '').replace('\\n', '')\n",
    "        chunk_data = chunk_data.split(':')[1]\n",
    "        chunk_data = chunk_data.replace('}', '')\n",
    "        chunk_data = chunk_data.replace('{', '')\n",
    "        chunk_data = chunk_data.replace('\"', '')\n",
    "        print(chunk_data, end=\"\", flush=True)\n",
    "\n",
    "    except (IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing chunk: {e}\")\n",
    "        chunk_dict = {\"data\": {\"text\": \"\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def get_llamaparse_rects(file_path, retrieved_node, page_nb):\n",
    "    doc = fitz.open(file_path)\n",
    "    page = doc[page_nb]\n",
    "    page_width = page.rect.width\n",
    "    page_height = page.rect.height\n",
    "    llamaparse_rects = retrieved_node.metadata.get(\"bounding_boxes\")\n",
    "    standard_rects = []\n",
    "    for rect in llamaparse_rects:\n",
    "        standard_rects.append({\n",
    "                    \"left\": float(rect.get(\"x\") / page_width) * 100,\n",
    "                    \"top\": float(rect.get(\"y\") / page_height) * 100,\n",
    "                    \"width\": float(rect.get(\"w\") / page_width) * 100,\n",
    "                    \"height\": float(rect.get(\"h\") / page_height) * 100,\n",
    "                    \"page\": page_nb + 1 # 1-indexed convention \n",
    "                } )\n",
    "    return standard_rects\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    ToolMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    AIMessageChunk,\n",
    ")\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph.message import add_messages\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from typing import Annotated, TypedDict, List, Iterator, AsyncIterator\n",
    "from config import LLM, TAVILY_TOOL\n",
    "import re\n",
    "import json \n",
    "from rapidfuzz.fuzz import partial_ratio\n",
    "from boundig_boxes import get_standard_rects_from_pdf, prune_overlapping_rects\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "\n",
    "\n",
    "class ToolRetrievedNode(BaseModel):\n",
    "    metadata: dict\n",
    "    text: str\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"\n",
    "    Runs the tools requested in the last AIMessage.\n",
    "\n",
    "    Updates the state with the retrieved nodes from the tool calls.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "\n",
    "        # Get the last message\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "\n",
    "        # Get the tool calls\n",
    "        for tool_call in message.tool_calls:\n",
    "\n",
    "            # Invoke the tool\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "\n",
    "            # Try to get sources from the tool result\n",
    "            has_retrieved_nodes = False\n",
    "            try:\n",
    "                retrieved_nodes_list = []\n",
    "                for retrieved_node in tool_result.raw_output.source_nodes:\n",
    "                    retrieved_nodes_list.append(ToolRetrievedNode(\n",
    "                        metadata=retrieved_node.metadata,\n",
    "                        text=retrieved_node.text\n",
    "                    ))\n",
    "                content = json.dumps(tool_result.content)\n",
    "                has_retrieved_nodes = True\n",
    "            except:\n",
    "                content = json.dumps(tool_result)\n",
    "\n",
    "            # Add the tool message to the outputs\n",
    "            outputs.append(ToolMessage(\n",
    "                content=content,\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            ))\n",
    "        \n",
    "        # if there are retrieved nodes, add them to the state\n",
    "        if has_retrieved_nodes:\n",
    "            return {\n",
    "                \"messages\": outputs,\n",
    "                \"retrieved_nodes\": retrieved_nodes_list,\n",
    "                \"execution_plan_steps\": inputs.get(\"execution_plan_steps\", []),\n",
    "                \"current_step\": inputs.get(\"current_step\", \"\")\n",
    "            }\n",
    "        return {\n",
    "            \"messages\": outputs,\n",
    "            \"retrieved_nodes\": inputs.get(\"retrieved_nodes\", []),\n",
    "            \"execution_plan_steps\": inputs.get(\"execution_plan_steps\", []),\n",
    "            \"current_step\": inputs.get(\"current_step\", \"\")\n",
    "        }\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    retrieved_nodes: list[list[any]]\n",
    "    full_answer: str\n",
    "\n",
    "\n",
    "class AsyncToolNode:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = BasicToolNode(tools)\n",
    "\n",
    "    async def __call__(self, state: GraphState) -> dict:\n",
    "        return self.tools(state)\n",
    "\n",
    "def sanitize_tool_name(name):\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", name)\n",
    "\n",
    "\n",
    "class BoundingBox(BaseModel):\n",
    "    left: float\n",
    "    top: float\n",
    "    width: float\n",
    "    height: float\n",
    "    page: int\n",
    "\n",
    "class HighlightArea(BaseModel):\n",
    "    bounding_boxes: list[BoundingBox]\n",
    "    jump_to_page_number: int\n",
    "\n",
    "class ChatSourceComment(BaseModel):\n",
    "    id: str\n",
    "    file_id: str\n",
    "    thread_id: str\n",
    "    message_id: str\n",
    "    text: str\n",
    "    highlight_area: HighlightArea\n",
    "    next_chat_comment_id: str\n",
    "\n",
    "class ChatAgentGraph:\n",
    "    \"\"\"\n",
    "    ChatAgentGraph represents the agent behind the chat interface.\n",
    "    \n",
    "    This agent can be instantiated with a list of tools. After instantiating the agent, you must call the `build()` method.\n",
    "\n",
    "    ```\n",
    "    agent = ChatAgentGraph(tools=[tool1, tool2, tool3])\n",
    "    agent.build()\n",
    "    ```\n",
    "\n",
    "    After building the agent, you can use the `stream()` method to start the chat. This method requires two arguments:\n",
    "    - `user_message`: The message to send to the agent.\n",
    "    - `thread_id`: The id of the thread to use. This is used to identify the thread in the database.\n",
    "\n",
    "    ```\n",
    "    async for message_chunk in agent.stream(user_message=\"Hello, how are you?\", thread_id=\"1\"):\n",
    "        print(message_chunk)\n",
    "    ```\n",
    "\n",
    "    The `stream()` method acts as an EventSource stream. It outputs events with the following format:\n",
    "\n",
    "    ```\n",
    "    data: {\"text\": \"Hello, how are you?\"}\n",
    "    event: \"message\"\n",
    "\n",
    "\n",
    "    ```\n",
    "\n",
    "    Currently, the stream supports the following events:\n",
    "    - \"message\": A message from the agent. Data will have the following format:\n",
    "    ```\n",
    "    data: {\"text\": \"Hello, how are you?\"}\n",
    "    event: \"message\"\n",
    "    ```\n",
    "    - \"sources\": A list of source annotations. Data will have the following format:\n",
    "    ```\n",
    "    data: [ChatSourceComment.model_dump(), ChatSourceComment.model_dump(), ...]\n",
    "    event: \"sources\"\n",
    "    ```\n",
    "    - \"complete\": The stream is complete. Data will have the following format:\n",
    "    ```\n",
    "    data: \"n.a.\"\n",
    "    event: \"complete\"\n",
    "    ```\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, tools: List[any]):\n",
    "        self.tools = []\n",
    "        for tool in tools:\n",
    "            if isinstance(tool, BaseTool):\n",
    "                self.tools.append(tool)\n",
    "            elif isinstance(tool, QueryEngineTool):\n",
    "                self.tools.append(tool.to_langchain_tool())\n",
    "            else:\n",
    "                try:\n",
    "                    self.tools.append(tool.to_langchain_tool())\n",
    "                except:\n",
    "                    raise ValueError(f\"Tool {tool} is not a recognized tool.\")\n",
    "        for tool in self.tools:\n",
    "            tool.name = sanitize_tool_name(tool.name)\n",
    "        self.tools_descriptions_string = \"\\n - \" + \"\\n -\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "        self.LLM = LLM\n",
    "        self.TOOL_LLM = LLM.bind_tools(self.tools)\n",
    "        self.graph = None\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    async def _communicate_plan(self, state: GraphState) -> AsyncIterator[dict]:\n",
    "        prompt = [\n",
    "            SystemMessage(content=f\"\"\"\n",
    "            You are an autonomous AI agent solving a task step-by-step using tools.\n",
    "            Decide what to do next. YOU MUST BASE YOUR ANSWER ON THE TOOLS PROVIDED BELOW. DO NOT RELY ON PRIOR KNOWLEDGE.\n",
    "                        \n",
    "            WRITE A SHORT AND CONCISE PLAN LIKE. \"Reading [Document Source] on [Company] from [date] to verify [Claim]...\"\n",
    "                        \n",
    "            {self.tools_descriptions_string}\n",
    "            \"\"\"),\n",
    "            *state[\"messages\"], \n",
    "            HumanMessage(content=\"Provide a plan for your next step.\")\n",
    "        ]\n",
    "\n",
    "        final = \"\"\n",
    "        async for chunk in self.LLM.astream(prompt):\n",
    "            delta = chunk.content or \"\"\n",
    "            final += delta\n",
    "        yield {\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=final)],\n",
    "            \"retrieved_nodes\": state.get(\"retrieved_nodes\", []),\n",
    "            \"full_answer\": state.get(\"full_answer\", \"\") + final,\n",
    "        }\n",
    "\n",
    "\n",
    "    async def _call_tools(self, state: GraphState) -> AsyncIterator[dict]:\n",
    "        prompt = [\n",
    "            SystemMessage(content=f\"\"\"\n",
    "            Only use the tools you have been provided with. \n",
    "            Base yourself on the plan provided by the user.\n",
    "            If the plan does not require you to use any tools. Don't do anything.\n",
    "                          \n",
    "            ONLY USE THE TOOLS YOU HAVE BEEN PROVIDED WITH.\n",
    "            \"\"\"),\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "        return {\n",
    "            \"messages\": [await self.TOOL_LLM.ainvoke(prompt)],\n",
    "            \"retrieved_nodes\": state[\"retrieved_nodes\"],\n",
    "            \"full_answer\": state[\"full_answer\"],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _custom_tools_condition(state: GraphState) -> str:\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "            return \"tools\"\n",
    "        else:\n",
    "            return \"no_tools\"\n",
    "\n",
    "\n",
    "    async def _communicate_output(self, state: GraphState) -> AsyncIterator[dict]:\n",
    "        prompt = [\n",
    "            SystemMessage(content=f\"\"\"\n",
    "            You will summarize an answer to the last Human Message based on the subsequent tool calls and AI messages\n",
    "            \"\"\"),\n",
    "            *state[\"messages\"]\n",
    "        ]\n",
    "        final = \"\"\n",
    "        async for chunk in self.LLM.astream(prompt):\n",
    "            delta = chunk.content or \"\"\n",
    "            final += delta\n",
    "\n",
    "        yield {\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=final)],\n",
    "            \"retrieved_nodes\": state.get(\"retrieved_nodes\", []),\n",
    "            \"full_answer\": state.get(\"full_answer\", \"\") + final,\n",
    "        }\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        # graph builder\n",
    "        graph_builder = StateGraph(GraphState)\n",
    "\n",
    "        # nodes\n",
    "        graph_builder.add_node(\"communicate_plan\", self._communicate_plan)\n",
    "        graph_builder.add_node(\"call_tools\", self._call_tools)\n",
    "        graph_builder.add_node(\"tools\", AsyncToolNode(tools=self.tools))\n",
    "\n",
    "        # fixed edges\n",
    "        graph_builder.add_edge(START, \"communicate_plan\")\n",
    "        graph_builder.add_edge(\"communicate_plan\", \"call_tools\")\n",
    "        graph_builder.add_edge(\"tools\", \"call_tools\")\n",
    "\n",
    "        # conditional edges\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"call_tools\",\n",
    "            self._custom_tools_condition,\n",
    "            {\"tools\":\"tools\", \"no_tools\":END}\n",
    "        )\n",
    "\n",
    "        # compile\n",
    "        graph = graph_builder.compile(checkpointer=MemorySaver())\n",
    "        self.graph = graph\n",
    "\n",
    "        return graph\n",
    "    \n",
    "\n",
    "    def _get_retrieved_nodes(self):\n",
    "\n",
    "        # extract nodes which were retrieved during the last run through the graph\n",
    "        retrieved_nodes = self.graph.get_state(self.config).values.get(\"retrieved_nodes\", [])\n",
    "\n",
    "        # prune nodes which have similar text\n",
    "        pruned_retrieved_nodes = []\n",
    "        for retrieved_node in retrieved_nodes:\n",
    "            retrieved_node_text = retrieved_node.text.strip().lower()\n",
    "            for pruned_retrieved_node in pruned_retrieved_nodes:\n",
    "                pruned_retrieved_node_text = pruned_retrieved_node.text.strip().lower()\n",
    "                if partial_ratio(retrieved_node_text, pruned_retrieved_node_text) > 90:\n",
    "                    break\n",
    "            else:\n",
    "                pruned_retrieved_nodes.append(retrieved_node)\n",
    "        \n",
    "        return pruned_retrieved_nodes\n",
    "\n",
    "    def _get_chat_source_comments(self):\n",
    "\n",
    "        chat_source_comments = []\n",
    "\n",
    "        # get the retrieved nodes from the graph\n",
    "        retrieved_nodes = self._get_retrieved_nodes()\n",
    "        nb_messages = len(self.graph.get_state(self.config).values.get(\"messages\", []))\n",
    "\n",
    "        # convert the retrieved nodes to source annotations\n",
    "        last_unique_id = \"\"\n",
    "        for retrieved_node in retrieved_nodes:\n",
    "            file_path = retrieved_node.metadata.get(\"file_path\")\n",
    "            text = retrieved_node.text\n",
    "            page_nb = retrieved_node.metadata.get(\"page_nb\")\n",
    "\n",
    "            # get the rects from the llama parse tool (course)\n",
    "            backup_standard_rects = get_llamaparse_rects(file_path, retrieved_node, page_nb)\n",
    "\n",
    "            # get the rects based on text from the pdf (fine-grained)\n",
    "            standard_rects = get_standard_rects_from_pdf(file_path, text, page_nb)\n",
    "            standard_rects = prune_overlapping_rects(standard_rects)\n",
    "\n",
    "            # if no fine-grained rects are found, use the course rects\n",
    "            if len(standard_rects) == 0:\n",
    "                print(\"[harlus_chat] No fine-grained rects found, using course rects\")\n",
    "                standard_rects = backup_standard_rects\n",
    "\n",
    "            # convert to ChatSourceComment framework\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            bboxes = [BoundingBox(**rect) for rect in standard_rects]\n",
    "            highlight_area = HighlightArea(bounding_boxes=bboxes, jump_to_page_number=page_nb)\n",
    "            chat_source_comment = ChatSourceComment(\n",
    "                highlight_area=highlight_area,\n",
    "                id=unique_id,\n",
    "                file_id=file_path,\n",
    "                thread_id=self.config[\"configurable\"].get(\"thread_id\"),\n",
    "                message_id=str(nb_messages),\n",
    "                text=\"Response source\",\n",
    "                next_chat_comment_id=last_unique_id\n",
    "            )\n",
    "            last_unique_id = unique_id\n",
    "            chat_source_comments.append(chat_source_comment)\n",
    "\n",
    "        return chat_source_comments\n",
    "    \n",
    "\n",
    "    async def stream(self, user_message: str, thread_id: str = \"1\"):\n",
    "\n",
    "        input_state = {\n",
    "            \"messages\": [(\"user\", user_message)], \n",
    "            \"retrieved_nodes\": [], \n",
    "            \"full_answer\": \"\"\n",
    "        }\n",
    "\n",
    "        self.config[\"configurable\"][\"thread_id\"] = thread_id\n",
    "\n",
    "        # 1. stream the answer \n",
    "        print(\"[harlus_chat] Streaming answer...\")\n",
    "        async for message_chunk, metadata in self.graph.astream(\n",
    "            input_state,\n",
    "            stream_mode=\"messages\",\n",
    "            config = self.config\n",
    "        ):\n",
    "            try:\n",
    "                if isinstance(message_chunk, AIMessageChunk):\n",
    "                    response = '\\n'.join([\n",
    "                        f'data: {json.dumps({\"text\": message_chunk.content})}',\n",
    "                        f'event: {\"message\"}',\n",
    "                        '\\n\\n'\n",
    "                    ])\n",
    "                \n",
    "                    yield response\n",
    "            except Exception as e:\n",
    "                print(f\"Streaming error: {e}\")\n",
    "            \n",
    "        # 2. stream the source annotations\n",
    "        print(\"[harlus_chat] Streaming source annotations...\")\n",
    "        try:\n",
    "            data = self._get_chat_source_comments()\n",
    "            data = [d.model_dump() for d in data]\n",
    "            response = '\\n'.join([\n",
    "                    f'data: {json.dumps(data)}',\n",
    "                    f'event: {\"sources\"}',\n",
    "                    '\\n\\n'\n",
    "            ])\n",
    "            print(f\"[harlus_chat] Sent {len(data)} source annotations\")\n",
    "            yield response\n",
    "        except Exception as e:\n",
    "            print(f\"[harlus_chat] Error sending source annotations: {e}\")\n",
    "\n",
    "        # 3. stream the completion\n",
    "        response = '\\n'.join([\n",
    "                    f'data: ',\n",
    "                    f'event: {\"complete\"}',\n",
    "                    '\\n\\n'\n",
    "            ])\n",
    "        yield response\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating nodes for /Users/ronald/Documents/code/harlus/ml/tools/harlus_chat/attributes/test1.pdf ...\n",
      " - parsing PDF to JSON...\n",
      "Started parsing the file under job_id c655d5e7-2348-47f0-bef3-c6ccb31ff767\n",
      " - creating nodes from JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 26250.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding node relationships...\n",
      " - splitting tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  > splitting tables:   0%|          | 0/41 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:00, 237.01it/s]\n",
      "1it [00:00, 257.07it/s]\n",
      "1it [00:00, 263.79it/s]\n",
      "1it [00:00, 166.86it/s]\n",
      "1it [00:00, 79.66it/s]\n",
      "1it [00:00, 70.59it/s]\n",
      "  > splitting tables: 100%|██████████| 41/41 [00:01<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding metadata to nodes...\n",
      "  > extracting metadata for SummaryExtractor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:03<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > extracting metadata for KeywordExtractor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - splitting text nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  > splitting text nodes: 100%|██████████| 168/168 [00:00<00:00, 5628.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding file path to nodes...\n",
      "getting single doc query engine from nodes\n",
      " - building vector index ...\n",
      " - building vector retriever ...\n",
      " - building keyword index ...\n",
      " - building summary index ...\n",
      " - building keyword retriever ...\n",
      " - building mix keyword vector retriever ...\n",
      " - building recursive keyword vector retriever ...\n",
      " - building mix keyword vector retriever query engine ...\n",
      " - building summary index query engine...\n",
      " - extracting metadata from query engines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata: 100%|██████████| 7/7 [00:16<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - building mix retriever query engine tool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[harlus_chat] Streaming answer...\n",
      "  Reading  the  Form   10 -K  for  Apple  Inc .  for  the  fiscal  year  ended  September   28 ,   202 4 ,  to  verify  if  Apple  has  a  business  segment  in  the  Americas .                   Yes ,  Apple  has  a  business  segment  in  the  Americas ,  which  includes  both  North  and  South  America . [harlus_chat] Streaming source annotations...\n",
      "[harlus_chat] Sent 1 source annotations\n",
      " 9ea835cc-03ee-4b01-987a-1870556de6a2, file_idError processing chunk: list index out of range\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = gp._get_chat_source_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatSourceComment(id='5c678fca-e1f2-428f-b04e-734824db2f1b', file_id='/Users/ronald/Documents/code/harlus/ml/tools/harlus_chat/attributes/test1.pdf', thread_id='1', message_id='5', text='Response source', highlight_area=HighlightArea(bounding_boxes=[BoundingBox(left=8.20625241063222, top=56.32648603933158, width=83.58697073800224, height=0.9763196641645636, page=5), BoundingBox(left=8.20625241063222, top=57.46550933765402, width=83.58691944795497, height=0.9763196641645636, page=5), BoundingBox(left=8.20625241063222, top=58.68592250941768, width=83.58710409212513, height=0.9763196641645636, page=5), BoundingBox(left=8.20625241063222, top=59.82494580774013, width=83.5875554445411, height=0.9763196641645636, page=5), BoundingBox(left=8.20625241063222, top=60.964001725801666, width=83.58652964359572, height=0.9763160397491092, page=5), BoundingBox(left=8.20625241063222, top=62.18438590224169, width=74.93465263302586, height=0.9763160397491092, page=5)], jump_to_page_number=5), next_chat_comment_id='')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
