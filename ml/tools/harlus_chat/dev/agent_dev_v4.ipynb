{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from doc_search import DocToolLoader\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating nodes for /Users/ronald/Documents/code/harlus/ml/agents/harlus_chat/attributes/test1.pdf ...\n",
      " - parsing PDF to JSON...\n",
      "Started parsing the file under job_id 0c21f119-c775-4968-ae36-1916e29ef219\n",
      " - creating nodes from JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 15507.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding node relationships...\n",
      " - splitting tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  > splitting tables:   0%|          | 0/38 [00:00<?, ?it/s]\n",
      "1it [00:00, 705.76it/s]\n",
      "1it [00:00, 1072.71it/s]\n",
      "1it [00:00, 28339.89it/s]\n",
      "  > splitting tables: 100%|██████████| 38/38 [00:01<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding metadata to nodes...\n",
      "  > extracting metadata for SummaryExtractor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:03<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > extracting metadata for KeywordExtractor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:01<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - splitting text nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  > splitting text nodes: 100%|██████████| 156/156 [00:00<00:00, 5323.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - adding file path to nodes...\n",
      "getting single doc query engine from nodes\n",
      " - building vector index ...\n",
      " - building vector retriever ...\n",
      " - building keyword index ...\n",
      " - building summary index ...\n",
      " - building keyword retriever ...\n",
      " - building mix keyword vector retriever ...\n",
      " - building recursive keyword vector retriever ...\n",
      " - building mix keyword vector retriever query engine ...\n",
      " - building summary index query engine...\n",
      " - extracting metadata from query engines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata: 100%|██████████| 7/7 [00:17<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - building mix retriever query engine tool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tool_loader = await DocToolLoader().load(\n",
    "    file_path=\"/Users/ronald/Documents/code/harlus/ml/agents/harlus_chat/attributes/test1.pdf\",\n",
    "    file_name= \"test1\"\n",
    ")\n",
    "tool_1 = tool_loader.get().to_langchain_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    ToolMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    AIMessageChunk,\n",
    ")\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from typing import Annotated, TypedDict, List, Iterator, AsyncIterator\n",
    "from config import LLM, TAVILY_TOOL\n",
    "\n",
    "import json \n",
    "import orjson\n",
    "from harlus_chat.boundig_boxes import get_vertices, vertices_to_rects, rects_to_reactpdf\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "\n",
    "        # Get the last message\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "\n",
    "        # Get the tool calls\n",
    "        for tool_call in message.tool_calls:\n",
    "\n",
    "            # Invoke the tool\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "\n",
    "            # Try to get sources from the tool result\n",
    "            has_retrieved_nodes = False\n",
    "            try:\n",
    "                retrieved_nodes_list = []\n",
    "                for retrieved_node in tool_result.raw_output.source_nodes:\n",
    "                    retrieved_nodes_list.append({\n",
    "                        \"metadata\": retrieved_node.metadata,\n",
    "                        \"text\": retrieved_node.text\n",
    "                    })\n",
    "                content = json.dumps(tool_result.content)\n",
    "                has_retrieved_nodes = True\n",
    "            except:\n",
    "                content = json.dumps(tool_result)\n",
    "\n",
    "            # Add the tool message to the outputs\n",
    "            outputs.append(ToolMessage(\n",
    "                content=content,\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            ))\n",
    "        \n",
    "        # if there are retrieved nodes, add them to the state\n",
    "        if has_retrieved_nodes:\n",
    "            return {\n",
    "                \"messages\": outputs,\n",
    "                \"retrieved_nodes\": retrieved_nodes_list,\n",
    "                \"execution_plan_steps\": inputs.get(\"execution_plan_steps\", []),\n",
    "                \"current_step\": inputs.get(\"current_step\", \"\")\n",
    "            }\n",
    "        return {\n",
    "            \"messages\": outputs,\n",
    "            \"retrieved_nodes\": inputs.get(\"retrieved_nodes\", []),\n",
    "            \"execution_plan_steps\": inputs.get(\"execution_plan_steps\", []),\n",
    "            \"current_step\": inputs.get(\"current_step\", \"\")\n",
    "        }\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    retrieved_nodes: list[list[any]]\n",
    "    full_answer: str\n",
    "\n",
    "\n",
    "class AsyncToolNode:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = BasicToolNode(tools)\n",
    "\n",
    "    async def __call__(self, state: GraphState) -> dict:\n",
    "        return self.tools(state)\n",
    "\n",
    "\n",
    "\n",
    "class GraphPipeline:\n",
    "    def __init__(self, tools: List[any]):\n",
    "        self.tools = []\n",
    "        for tool in tools:\n",
    "            if isinstance(tool, BaseTool):\n",
    "                self.tools.append(tool)\n",
    "            elif isinstance(tool, QueryEngineTool):\n",
    "                self.tools.append(tool.to_langchain_tool())\n",
    "            else:\n",
    "                try:\n",
    "                    self.tools.append(tool.to_langchain_tool())\n",
    "                except:\n",
    "                    raise ValueError(f\"Tool {tool} is not a recognized tool.\")\n",
    "        self.tools_descriptions_string = \"\\n - \" + \"\\n -\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "        self.LLM = LLM\n",
    "        self.TOOL_LLM = LLM.bind_tools(self.tools)\n",
    "        self.graph = None\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    async def communicate_plan(self, state: GraphState) -> AsyncIterator[dict]:\n",
    "        prompt = [\n",
    "            SystemMessage(content=f\"\"\"\n",
    "            You are an autonomous AI agent solving a task step-by-step using tools.\n",
    "            Decide what to do next. YOU MUST BASE YOUR ANSWER ON THE TOOLS PROVIDED BELOW. DO NOT RELY ON PRIOR KNOWLEDGE.\n",
    "                        \n",
    "            WRITE A SHORT AND CONCISE PLAN LIKE. \"I will read the [Document Source] on [Company] from [date] to verify ...\"\n",
    "                        \n",
    "            {self.tools_descriptions_string}\n",
    "            \"\"\"),\n",
    "            *state[\"messages\"], \n",
    "            HumanMessage(content=\"Provide a plan for your next step.\")\n",
    "        ]\n",
    "\n",
    "        final = \"\"\n",
    "        async for chunk in self.LLM.astream(prompt):\n",
    "            delta = chunk.content or \"\"\n",
    "            final += delta\n",
    "            #yield {\n",
    "            #    \"messages\": [AIMessageChunk(content=delta)],\n",
    "            #    \"retrieved_nodes\": state.get(\"retrieved_nodes\", []),\n",
    "            #    \"full_answer\": state.get(\"full_answer\", \"\"),\n",
    "            #}\n",
    "\n",
    "        yield {\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=final)],\n",
    "            \"retrieved_nodes\": state.get(\"retrieved_nodes\", []),\n",
    "            \"full_answer\": state.get(\"full_answer\", \"\") + final,\n",
    "        }\n",
    "\n",
    "\n",
    "    async def call_tools(self, state: GraphState) -> AsyncIterator[dict]:\n",
    "        prompt = [\n",
    "            SystemMessage(content=f\"\"\"\n",
    "             Only use the toools you have been provided with. \n",
    "            Base yourself on the plan provided by the user.\n",
    "            If the plan does not require you to use any tools. Don't do anything.\n",
    "            \"\"\"),\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "        return {\n",
    "            \"messages\": [await self.TOOL_LLM.ainvoke(prompt)],\n",
    "            \"retrieved_nodes\": state[\"retrieved_nodes\"],\n",
    "            \"full_answer\": state[\"full_answer\"],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_tools_condition(state: GraphState) -> str:\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "            return \"tools\"\n",
    "        else:\n",
    "            return \"no_tools\"\n",
    "\n",
    "\n",
    "    async def communicate_output(self, state: GraphState) -> AsyncIterator[dict]:\n",
    "        prompt = [\n",
    "            SystemMessage(content=f\"\"\"\n",
    "            You will summarize an answer to the last Human Message based on the subsequent tool calls and AI messages\n",
    "            \"\"\"),\n",
    "            *state[\"messages\"]\n",
    "        ]\n",
    "        final = \"\"\n",
    "        async for chunk in self.LLM.astream(prompt):\n",
    "            delta = chunk.content or \"\"\n",
    "            final += delta\n",
    "            #yield {\n",
    "            #    \"messages\": [AIMessageChunk(content=delta)],\n",
    "            #    \"retrieved_nodes\": state.get(\"retrieved_nodes\", []),\n",
    "            #    \"full_answer\": state.get(\"full_answer\", \"\"),\n",
    "            #}\n",
    "\n",
    "        yield {\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=final)],\n",
    "            \"retrieved_nodes\": state.get(\"retrieved_nodes\", []),\n",
    "            \"full_answer\": state.get(\"full_answer\", \"\") + final,\n",
    "        }\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        # graph builder\n",
    "        graph_builder = StateGraph(GraphState)\n",
    "\n",
    "        # nodes\n",
    "        graph_builder.add_node(\"communicate_plan\", self.communicate_plan)\n",
    "        graph_builder.add_node(\"call_tools\", self.call_tools)\n",
    "        graph_builder.add_node(\"tools\", AsyncToolNode(tools=self.tools))\n",
    "\n",
    "        # fixed edges\n",
    "        graph_builder.add_edge(START, \"communicate_plan\")\n",
    "        graph_builder.add_edge(\"communicate_plan\", \"call_tools\")\n",
    "        graph_builder.add_edge(\"tools\", \"call_tools\")\n",
    "\n",
    "        # conditional edges\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"call_tools\",\n",
    "            self.custom_tools_condition,\n",
    "            {\"tools\":\"tools\", \"no_tools\":END}\n",
    "        )\n",
    "\n",
    "        # compile\n",
    "        graph = graph_builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "        self.graph = graph\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    async def stream_core(self, user_message: str, mode: str = \"messages\"):\n",
    "        \n",
    "        input_state = {\n",
    "            \"messages\": [(\"user\", user_message)], \n",
    "            \"retrieved_nodes\": [], \n",
    "            \"full_answer\": \"\"\n",
    "        }\n",
    "        seen = ()\n",
    "        async for message_chunk, metadata in self.graph.astream(\n",
    "            input_state,\n",
    "            stream_mode=mode,\n",
    "            config = self.config\n",
    "        ):\n",
    "            if isinstance(message_chunk, AIMessageChunk):\n",
    "                yield message_chunk.content\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    def get_retrieved_nodes(self):\n",
    "        retrieved_nodes = self.graph.get_state(self.config).values.get(\"retrieved_nodes\")\n",
    "        source_annotations = []\n",
    "        for retrieved_node in retrieved_nodes:\n",
    "            file_path = retrieved_node[\"metadata\"][\"file_path\"]\n",
    "            text = retrieved_node[\"text\"]\n",
    "            vertices = get_vertices(file_path, text)\n",
    "            rects = vertices_to_rects(vertices)\n",
    "            bboxes = rects_to_reactpdf(rects)\n",
    "            page_nb = retrieved_node[\"metadata\"][\"page_nb\"]\n",
    "            source_annotations.append({\n",
    "                \"file_path\": file_path,\n",
    "                \"page_nb\": page_nb,\n",
    "                \"bboxes\": bboxes\n",
    "            })\n",
    "        return source_annotations\n",
    "    \n",
    "\n",
    "    async def event_stream_generator(self, user_message: str, mode: str = \"messages\"):\n",
    "        input_state = {\n",
    "            \"messages\": [(\"user\", user_message)], \n",
    "            \"retrieved_nodes\": [], \n",
    "            \"full_answer\": \"\"\n",
    "        }\n",
    "        async for message_chunk, metadata in self.graph.astream(\n",
    "            input_state,\n",
    "            stream_mode=mode,\n",
    "            config = self.config\n",
    "        ):\n",
    "            if isinstance(message_chunk, AIMessageChunk):\n",
    "                response = '\\n'.join([\n",
    "                    f'data: {message_chunk.content}',\n",
    "                    f'event: {\"message\"}',\n",
    "                    '\\n\\n'\n",
    "                ])\n",
    "            \n",
    "                yield response\n",
    "        \n",
    "        data = self.get_retrieved_nodes()\n",
    "        response = '\\n'.join([\n",
    "                    f'data: {data}',\n",
    "                    f'event: {\"source_annotations\"}',\n",
    "                    '\\n\\n'\n",
    "        ])\n",
    "        yield response\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: I\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  will\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  read\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  the\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  \"\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: Annual\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Report\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  on\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Form\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: 10\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: -K\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  for\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  the\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Fiscal\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Year\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  End\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: ed\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  September\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: 28\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: ,\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: 202\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: 4\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \"\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  for\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Apple\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Inc\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: .\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  to\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  verify\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  if\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  the\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  company\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  has\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  a\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  business\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  segment\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  in\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  the\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Americas\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: .\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: Yes\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: ,\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Apple\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Inc\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: .\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  has\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  a\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  business\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  segment\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  in\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  the\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  Americas\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: ,\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  which\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  includes\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  both\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  North\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  and\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  South\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  America\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: ,\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  as\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  outlined\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  in\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  their\n",
      "event: message\n",
      "\n",
      "\n",
      "||data:  report\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: .\n",
      "event: message\n",
      "\n",
      "\n",
      "||data: \n",
      "event: message\n",
      "\n",
      "\n",
      "||data: [{'file_path': '/Users/ronald/Documents/code/harlus/ml/agents/harlus_chat/attributes/test1.pdf', 'page_nb': 5, 'bboxes': [{'left': 48.82720184326172, 'top': 474.2690124511719, 'width': 497.3424758911133, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 483.8595886230469, 'width': 497.34217071533203, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 494.1354675292969, 'width': 497.34326934814453, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 503.7260437011719, 'width': 410.5074234008789, 'height': 8.220611572265625}]}, {'file_path': '/Users/ronald/Documents/code/harlus/ml/agents/harlus_chat/attributes/test1.pdf', 'page_nb': 5, 'bboxes': [{'left': 48.82720184326172, 'top': 474.2690124511719, 'width': 497.3424758911133, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 483.8595886230469, 'width': 497.34217071533203, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 494.1354675292969, 'width': 206.15377044677734, 'height': 8.220611572265625}]}, {'file_path': '/Users/ronald/Documents/code/harlus/ml/agents/harlus_chat/attributes/test1.pdf', 'page_nb': 5, 'bboxes': [{'left': 48.82720184326172, 'top': 474.2690124511719, 'width': 497.3424758911133, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 483.8595886230469, 'width': 497.34217071533203, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 494.1354675292969, 'width': 497.34326934814453, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 503.7260437011719, 'width': 497.34595489501953, 'height': 8.220611572265625}, {'left': 48.82720184326172, 'top': 513.31689453125, 'width': 497.33985137939453, 'height': 8.2205810546875}, {'left': 48.82720184326172, 'top': 523.592529296875, 'width': 445.8611831665039, 'height': 8.2205810546875}]}]\n",
      "event: source_annotations\n",
      "\n",
      "\n",
      "||"
     ]
    }
   ],
   "source": [
    "gp = GraphPipeline([tool_1])\n",
    "g =  gp.build_graph()\n",
    "async for chunk in gp.event_stream_generator(\"Did apple have a business segment in the americas?\"):\n",
    "    print(chunk, end=\"||\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
