{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75846029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from src.harlus_contrast_tool import (\n",
    "    ContrastTool,\n",
    "    ClaimQueryEngineToolLoader,\n",
    "    VerdictQueryEngineToolLoader,\n",
    "    SentenceRetrieverToolLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd2ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id a96e72aa-367b-40cf-a718-40c9ffbdb46e\n",
      " - creating nodes from documents...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File C:\\\\Users\\\\info\\\\Desktop\\\\harlus\\\\ml\\\\tools\\\\contrast_tool\\\\data\\\\AMAT_Q3_2024_Earnings_Release.pdf does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m thesis_qengine = \u001b[38;5;28;01mawait\u001b[39;00m ClaimQueryEngineToolLoader().load(old_doc)\n\u001b[32m      5\u001b[39m thesis_sentence_retriever = \u001b[38;5;28;01mawait\u001b[39;00m SentenceRetrieverToolLoader().load(old_doc)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m update_qengine = \u001b[38;5;28;01mawait\u001b[39;00m VerdictQueryEngineToolLoader().load(new_doc)\n\u001b[32m      8\u001b[39m update_sentence_retriever = \u001b[38;5;28;01mawait\u001b[39;00m SentenceRetrieverToolLoader().load(new_doc)\n\u001b[32m     10\u001b[39m tool = ContrastTool()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\info\\Desktop\\harlus\\ml\\tools\\contrast_tool\\src\\harlus_contrast_tool\\loader.py:82\u001b[39m, in \u001b[36mVerdictQueryEngineToolLoader.load\u001b[39m\u001b[34m(self, file_path, unused_file_name)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     76\u001b[39m     file_path: \u001b[38;5;28mstr\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# TODO confirm file path exists\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     query_engine = \u001b[43mVerdictQueryEnginePipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaim checker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolWrapper(\n\u001b[32m     88\u001b[39m         query_engine,\n\u001b[32m     89\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_tool_name(),\n\u001b[32m     90\u001b[39m         {},  \u001b[38;5;66;03m# TODO: Populate this with debug info: parsed_text.json and json_nodes.json\u001b[39;00m\n\u001b[32m     91\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\info\\Desktop\\harlus\\ml\\tools\\contrast_tool\\src\\harlus_contrast_tool\\claim_checker.py:181\u001b[39m, in \u001b[36mVerdictQueryEnginePipeline.build\u001b[39m\u001b[34m(file_path, models_config, num_questions)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# llm that compares relevant data to claims\u001b[39;00m\n\u001b[32m    174\u001b[39m verification_llm = OpenAI(\n\u001b[32m    175\u001b[39m     model=models_config[\u001b[33m\"\u001b[39m\u001b[33mverification model\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    176\u001b[39m     temperature=models_config[\u001b[33m\"\u001b[39m\u001b[33mverification model\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    177\u001b[39m     max_tokens=models_config[\u001b[33m\"\u001b[39m\u001b[33mverification model\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    178\u001b[39m     system_prompt=PROMPT_VERDICT_TEXT,\n\u001b[32m    179\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m retriever = \u001b[43mVerdictQueryEnginePipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# node_postprocessors = [\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m#     # MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m#     LLMRerank(choice_batch_size=15, top_n=8, llm=self.question_llm),\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m - building retriever query engine ...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\info\\Desktop\\harlus\\ml\\tools\\contrast_tool\\src\\harlus_contrast_tool\\claim_checker.py:131\u001b[39m, in \u001b[36mVerdictQueryEnginePipeline.build_retriever\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m    127\u001b[39m node_parser = DoclingNodeParser()\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# nodes = node_parser.get_nodes_from_documents(documents)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m documents = \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_extractor\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_parser\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.load_data()\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# doc.metadata[\"document_type\"] = document_type\u001b[39;00m\n\u001b[32m    138\u001b[39m     doc.metadata[\u001b[33m\"\u001b[39m\u001b[33mfile_type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mpdf\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\info\\Desktop\\harlus\\python\\env\\.venv\\Lib\\site-packages\\llama_index\\core\\readers\\file\\base.py:283\u001b[39m, in \u001b[36mSimpleDirectoryReader.__init__\u001b[39m\u001b[34m(self, input_dir, input_files, exclude, exclude_hidden, exclude_empty, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m input_files:\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fs.isfile(path):\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    284\u001b[39m     input_file = _Path(path)\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_files.append(input_file)\n",
      "\u001b[31mValueError\u001b[39m: File C:\\\\Users\\\\info\\\\Desktop\\\\harlus\\\\ml\\\\tools\\\\contrast_tool\\\\data\\\\AMAT_Q3_2024_Earnings_Release.pdf does not exist."
     ]
    }
   ],
   "source": [
    "old_doc = r\"C:\\\\Users\\\\info\\\\Desktop\\\\harlus\\\\ml\\\\tools\\\\contrast_tool\\\\data\\\\AMAT_Q1_24_results.pdf\"\n",
    "new_doc = r\"C:\\\\Users\\\\info\\\\Desktop\\\\harlus\\\\ml\\\\tools\\\\contrast_tool\\\\data\\\\AMAT_Q3_2024_Earnings_Release.pdf\"\n",
    "\n",
    "thesis_qengine = await ClaimQueryEngineToolLoader().load(old_doc)\n",
    "thesis_sentence_retriever = await SentenceRetrieverToolLoader().load(old_doc)\n",
    "\n",
    "update_qengine = await VerdictQueryEngineToolLoader().load(new_doc)\n",
    "update_sentence_retriever = await SentenceRetrieverToolLoader().load(new_doc)\n",
    "\n",
    "tool = ContrastTool()\n",
    "\n",
    "comments = tool.run(\n",
    "    old_doc,\n",
    "    thesis_qengine.get(),\n",
    "    thesis_sentence_retriever.get(),\n",
    "    new_doc,\n",
    "    update_qengine.get(),\n",
    "    update_sentence_retriever.get(),\n",
    ")\n",
    "\n",
    "for comment in comments:\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feaf5e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClaimComment(file_path='C:\\\\Users\\\\info\\\\Desktop\\\\harlus\\\\ml\\\\tools\\\\contrast_tool\\\\data\\\\AMAT Q1 24 results.pdf', text='The claim states that AMAT guided for sales of $6.65 billion for Q3 2024, with a year-over-year growth of 3.5%. However, the context indicates that the actual revenue for Q3 2024 was $6.78 billion, representing a 5% year-over-year growth. The claim understates both the sales figure and the growth rate. The percent error for the sales figure is |(6.65 - 6.78) / 6.78| x 100% = 1.9%, and for the growth rate, it is |(3.5 - 5) / 5| x 100% = 30%.', highlight_area=HighlightArea(bounding_boxes=[BoundingBox(left=11.317647048850464, top=86.33916064946338, width=7.168923172296263, height=1.5241834852430556, page=1), BoundingBox(left=23.513070898118364, top=86.33916064946338, width=66.35881311753216, height=1.5241834852430556, page=1), BoundingBox(left=23.513070898118364, top=87.86946306324968, width=7.370730630712571, height=1.5241834852430556, page=1)], jump_to_page=1), links=[LinkComment(file_path='C:\\\\Users\\\\info\\\\Desktop\\\\harlus\\\\ml\\\\tools\\\\contrast_tool\\\\data\\\\AMAT Q3 2024 Earnings Release.pdf', highlight_area=HighlightArea(bounding_boxes=[BoundingBox(left=44.957518733404825, top=42.07673506303267, width=44.71078511157067, height=1.5380859375, page=1), BoundingBox(left=8.088235294117647, top=43.74340327099116, width=80.92647377961602, height=1.5380859375, page=1)], jump_to_page=1), text=None)], verdict='false')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
