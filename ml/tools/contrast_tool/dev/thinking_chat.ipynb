{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Generator, Iterable\n",
    "\n",
    "import pickle\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage, AIMessage, HumanMessage, BaseMessage\n",
    "from langchain.agents import Tool\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "#  CONFIG \n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "DEBUG = False            # planner + high‑level debug\n",
    "DEBUG_VERBOSE_RAW = False  # print raw tool outputs (can be large)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "NumExpr defaulting to 16 threads.\n",
      "c:\\Users\\info\\Desktop\\harlus\\python\\env\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_30312\\3450132703.py:64: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# STATE\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class CompareState:\n",
    "    \"\"\"Holds everything the graph knows so far.\"\"\"\n",
    "\n",
    "    user_prompt: str  # original complex prompt from the user\n",
    "    conversation: List[Dict[str, str]] = field(default_factory=list)\n",
    "\n",
    "    # Working memory --------------------------------------------------------\n",
    "    claims: List[str] = field(default_factory=list)  # extracted from doc‑A\n",
    "    claim_status: Dict[str, str] = field(default_factory=dict)  # claim -> status\n",
    "    claim_attempts: Dict[str, int] = field(default_factory=dict)  # claim -> tries\n",
    "    new_drivers: List[str] = field(default_factory=list)  # extracted from doc‑B\n",
    "\n",
    "    # Control ---------------------------------------------------------------\n",
    "    next_action: Optional[Dict[str, str]] = None  # {tool, query, result?}\n",
    "    finished: bool = False  # when True the planner will route to END\n",
    "    steps: int = 0  # safeguard counter to avoid infinite loops\n",
    "\n",
    "    # Debug trace -----------------------------------------------------------\n",
    "    trace: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# TOOLS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _wrap_query_engine(name: str, engine):\n",
    "    \"\"\"Turns a LlamaIndex QueryEngineTool into a LangChain Tool.\"\"\"\n",
    "\n",
    "    def _run(query: str) -> str:  # synchronous wrapper; adapt if async\n",
    "        response = engine.query(query)\n",
    "        return str(response)\n",
    "\n",
    "    return Tool(name=name, func=_run, description=f\"Queries {name}\")\n",
    "\n",
    "\n",
    "with open(r\"C:\\Users\\info\\Desktop\\harlus\\server\\data\\AAPL\\Investment_Case_AMAT_pdf\\tools\\doc_search\\tool.pkl\", \"rb\") as file:\n",
    "    doc_tool_thesis = pickle.load(file)\n",
    "with open(r\"C:\\Users\\info\\Desktop\\harlus\\server\\data\\AAPL\\AMAT_Q3_24_Earnings_Release_pdf\\tools\\doc_search\\tool.pkl\", \"rb\") as file:\n",
    "    doc_tool_evidence = pickle.load(file)\n",
    "\n",
    "query_engine_a = doc_tool_thesis.tool.query_engine\n",
    "query_engine_b = doc_tool_evidence.tool.query_engine\n",
    "\n",
    "if query_engine_a is Ellipsis or query_engine_b is Ellipsis:\n",
    "    raise ValueError(\"Please supply `query_engine_a` and `query_engine_b`.\")\n",
    "\n",
    "predictions_tool = _wrap_query_engine(\"predictions_doc\", query_engine_a)\n",
    "newinfo_tool = _wrap_query_engine(\"new_info_doc\", query_engine_b)\n",
    "\n",
    "TOOLS_BY_NAME = {\n",
    "    predictions_tool.name: predictions_tool,\n",
    "    newinfo_tool.name: newinfo_tool,\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\", \n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "PLAN_SYS_PROMPT = (\n",
    "    \"You are the planning module of an autonomous agent that compares two \"\n",
    "    \"financial documents. Decide the *single* next action. Return **ONLY** a \"\n",
    "    \"JSON dict with keys: 'tool' (predictions_doc | new_info_doc | none) and \"\n",
    "    \"'query'. Do not wrap in ``` fences.\"\n",
    ")\n",
    "\n",
    "plan_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", PLAN_SYS_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _extract_json(text: str) -> Optional[Any]:\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    text = re.sub(r\"^```[a-zA-Z0-9]*|```$\", \"\", text.strip(), flags=re.MULTILINE)\n",
    "    m = re.search(r\"(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\", text)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(1))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _normalise_status(raw: str) -> Optional[str]:\n",
    "    raw = raw.lower()\n",
    "    if \"confirm\" in raw:\n",
    "        return \"confirmed\"\n",
    "    if any(k in raw for k in (\"contradict\", \"refut\", \"disagree\")):\n",
    "        return \"contradicted\"\n",
    "    if any(k in raw for k in (\"not_discuss\", \"not discuss\", \"irreleva\", \"no evidence\")):\n",
    "        return \"not_discussed\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _memory_as_messages(state: CompareState) -> List[BaseMessage]:\n",
    "    msgs: List[BaseMessage] = []\n",
    "    for it in state.conversation:\n",
    "        line = (\n",
    "            f\"TOOL {it['tool']} ⇒ {it['query'][:60].strip()}… :: \"\n",
    "            f\"{it['result'][:120].strip()}\"\n",
    "        )\n",
    "        msgs.append(SystemMessage(content=line))\n",
    "    if not msgs:\n",
    "        msgs.append(SystemMessage(content=\"⟨no prior actions⟩\"))\n",
    "    return msgs\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GRAPH NODES\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _llm_plan(state: CompareState) -> Optional[Dict[str, str]]:\n",
    "    memory_msgs = _memory_as_messages(state)\n",
    "    msgs = plan_prompt.format_messages(memory=memory_msgs)\n",
    "    resp = llm(msgs)\n",
    "    payload = _extract_json(resp.content)\n",
    "    if isinstance(payload, dict):\n",
    "        tool = payload.get(\"tool\")\n",
    "        query = payload.get(\"query\")\n",
    "        if tool in TOOLS_BY_NAME or tool == \"none\":\n",
    "            return {\"tool\": tool, \"query\": query}\n",
    "    return None\n",
    "\n",
    "\n",
    "def _fallback_plan(state: CompareState) -> Dict[str, str]:\n",
    "    if not state.claims:\n",
    "        return {\"tool\": predictions_tool.name, \"query\": (\n",
    "            \"Return ONLY a JSON array of distinct claims/hypotheses from the document.\"\n",
    "        )}\n",
    "    for cl in state.claims:\n",
    "        if cl not in state.claim_status and state.claim_attempts.get(cl, 0) < 3:\n",
    "            return {\"tool\": newinfo_tool.name, \"query\": (\n",
    "                f\"Does the document CONFIRM, CONTRADICT, or NOT DISCUSS this claim?\"\n",
    "                f\" Respond with JSON dict {{'status':...,'justification':...}} Claim: {cl}\"\n",
    "            )}\n",
    "    if not state.new_drivers:\n",
    "        joined = \"\\n\".join(state.claims)\n",
    "        return {\"tool\": newinfo_tool.name, \"query\": (\n",
    "            f\"List NEW drivers in this doc not in list:\\n{joined}\\nReturn JSON array only.\"\n",
    "        )}\n",
    "    return {\"tool\": \"none\", \"query\": \"\"}\n",
    "\n",
    "\n",
    "def planner_node(state: CompareState) -> CompareState:\n",
    "    if state.steps >= 60:\n",
    "        state.finished = True\n",
    "        return state\n",
    "    act = _llm_plan(state) or _fallback_plan(state)\n",
    "    if act[\"tool\"] == \"none\":\n",
    "        state.finished = True\n",
    "        state.next_action = None\n",
    "    else:\n",
    "        state.next_action = act\n",
    "    return state\n",
    "\n",
    "\n",
    "def run_tool_node(state: CompareState) -> CompareState:\n",
    "    spec = state.next_action\n",
    "    if not spec or spec.get(\"tool\") == \"none\":\n",
    "        return state\n",
    "    tool = TOOLS_BY_NAME[spec[\"tool\"]]\n",
    "    res = tool.run(spec[\"query\"])\n",
    "    state.conversation.append({\"tool\": tool.name, \"query\": spec[\"query\"], \"result\": res})\n",
    "    state.conversation = state.conversation[-6:]\n",
    "    state.trace.append({\"step\": state.steps, \"tool\": tool.name,\n",
    "                        \"query\": spec[\"query\"], \"raw\": res})\n",
    "    state.next_action[\"result\"] = res\n",
    "    return state\n",
    "\n",
    "\n",
    "def parse_update_node(state: CompareState) -> CompareState:\n",
    "    spec = state.next_action\n",
    "    if not spec or \"result\" not in spec:\n",
    "        return state\n",
    "    payload = _extract_json(spec[\"result\"])\n",
    "    if spec[\"tool\"] == predictions_tool.name and isinstance(payload, list):\n",
    "        state.claims = [str(c).strip() for c in payload if str(c).strip()]\n",
    "    elif spec[\"tool\"] == newinfo_tool.name:\n",
    "        if isinstance(payload, dict):\n",
    "            status = _normalise_status(str(payload.get(\"status\", \"\")))\n",
    "            cl = spec[\"query\"].split(\"Claim:\")[-1].strip()\n",
    "            if status:\n",
    "                state.claim_status[cl] = status\n",
    "            else:\n",
    "                state.claim_attempts[cl] = state.claim_attempts.get(cl, 0) + 1\n",
    "                if state.claim_attempts[cl] >= 3:\n",
    "                    state.claim_status[cl] = \"not_discussed\"\n",
    "        elif isinstance(payload, list):\n",
    "            state.new_drivers = [str(x).strip() for x in payload if str(x).strip()]\n",
    "    # Clear next_action and bump step\n",
    "    state.next_action = None\n",
    "    state.steps += 1\n",
    "    return state\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GRAPH DEFINITION\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "graph = StateGraph(CompareState)\n",
    "\n",
    "graph.add_node(\"plan\", planner_node)\n",
    "graph.add_node(\"run_tool\", run_tool_node)\n",
    "graph.add_node(\"parse\", parse_update_node)\n",
    "\n",
    "graph.set_entry_point(\"plan\")\n",
    "\n",
    "# plan → run_tool or END\n",
    "graph.add_conditional_edges(\"plan\", lambda s: \"run_tool\" if not s.finished else END)\n",
    "# run_tool → parse → plan\n",
    "graph.add_edge(\"run_tool\", \"parse\")\n",
    "graph.add_edge(\"parse\", \"plan\")\n",
    "\n",
    "compare_chain = graph.compile()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HELPER TO GENERATE FINAL ANSWER\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _md_bullets(lst: List[str]) -> str:\n",
    "    return \"\\n\".join(f\"- {x}\" for x in lst) if lst else \"_None_\"\n",
    "\n",
    "def format_final_answer(state: CompareState) -> str:\n",
    "    confirmed = [c for c, st in state.claim_status.items() if st == \"confirmed\"]\n",
    "    contradicted = [c for c, st in state.claim_status.items() if st == \"contradicted\"]\n",
    "    undiscussed = [c for c, st in state.claim_status.items() if st == \"not_discussed\"]\n",
    "\n",
    "    return (\n",
    "        \"## Comparison Result\\n\\n\"  # H2 header\n",
    "        \"### 1. Claims confirmed by new information\\n\" + _md_bullets(confirmed) + \"\\n\\n\"\n",
    "        \"### 2. Claims contradicted by new information\\n\" + _md_bullets(contradicted) + \"\\n\\n\"\n",
    "        \"### 3. Claims not discussed in new information\\n\" + _md_bullets(undiscussed) + \"\\n\\n\"\n",
    "        \"### 4. New value drivers introduced in new information\\n\" + _md_bullets(state.new_drivers)\n",
    "    )\n",
    "\n",
    "\n",
    "def compare_documents(\n",
    "    user_prompt: str,\n",
    "    *,\n",
    "    recursion_limit: int = 100,\n",
    "    debug: bool | None = None,\n",
    ") -> str:\n",
    "    \"\"\"High‑level helper. Returns *only* the final formatted Markdown answer.\n",
    "\n",
    "    Set ``debug=True`` to enable stdout tracing (planner + tool calls).\"\"\"\n",
    "\n",
    "    global DEBUG\n",
    "    if debug is not None:\n",
    "        DEBUG = debug\n",
    "\n",
    "    initial_state = CompareState(user_prompt=user_prompt)\n",
    "    final_state = compare_chain.invoke(\n",
    "        initial_state,\n",
    "        config={\"recursion_limit\": recursion_limit},\n",
    "    )\n",
    "    return format_final_answer(final_state)\n",
    "\n",
    "\n",
    "# ---- 🆕 STREAMING / INTROSPECTION API -------------------------------------\n",
    "\n",
    "def compare_documents_stream(\n",
    "    user_prompt: str,\n",
    "    *,\n",
    "    recursion_limit: int = 100,\n",
    ") -> Iterable[CompareState]:\n",
    "    \"\"\"Generator that yields **every** intermediate state, so you can watch\n",
    "    progress or analyse after‑the‑fact.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        for st in compare_documents_stream(prompt):\n",
    "            print(st.steps, st.next_action)  # or log whatever\n",
    "    \"\"\"\n",
    "\n",
    "    initial_state = CompareState(user_prompt=user_prompt)\n",
    "    for chunk in compare_chain.stream(\n",
    "        initial_state,\n",
    "        config={\"recursion_limit\": recursion_limit},\n",
    "        stream_mode=\"values\",  # yields plain state values\n",
    "    ):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL ANSWER (quiet run) ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_30312\\3450132703.py:134: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resp = llm(msgs)\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AddableValuesDict' object has no attribute 'claim_status'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m prompt = (\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCompare document A (predictions) with document B (earnings report) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mand classify predictions as confirmed/contradicted/undiscussed, plus \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlist new drivers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== FINAL ANSWER (quiet run) ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcompare_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Verbose streaming --------------------------------------------------\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== STREAMED STEPS ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 274\u001b[39m, in \u001b[36mcompare_documents\u001b[39m\u001b[34m(user_prompt, recursion_limit, debug)\u001b[39m\n\u001b[32m    269\u001b[39m initial_state = CompareState(user_prompt=user_prompt)\n\u001b[32m    270\u001b[39m final_state = compare_chain.invoke(\n\u001b[32m    271\u001b[39m     initial_state,\n\u001b[32m    272\u001b[39m     config={\u001b[33m\"\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m\"\u001b[39m: recursion_limit},\n\u001b[32m    273\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformat_final_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 242\u001b[39m, in \u001b[36mformat_final_answer\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_final_answer\u001b[39m(state: CompareState) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     confirmed = [c \u001b[38;5;28;01mfor\u001b[39;00m c, st \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclaim_status\u001b[49m.items() \u001b[38;5;28;01mif\u001b[39;00m st == \u001b[33m\"\u001b[39m\u001b[33mconfirmed\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    243\u001b[39m     contradicted = [c \u001b[38;5;28;01mfor\u001b[39;00m c, st \u001b[38;5;129;01min\u001b[39;00m state.claim_status.items() \u001b[38;5;28;01mif\u001b[39;00m st == \u001b[33m\"\u001b[39m\u001b[33mcontradicted\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    244\u001b[39m     undiscussed = [c \u001b[38;5;28;01mfor\u001b[39;00m c, st \u001b[38;5;129;01min\u001b[39;00m state.claim_status.items() \u001b[38;5;28;01mif\u001b[39;00m st == \u001b[33m\"\u001b[39m\u001b[33mnot_discussed\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'AddableValuesDict' object has no attribute 'claim_status'"
     ]
    }
   ],
   "source": [
    "# Basic one‑shot -----------------------------------------------------\n",
    "prompt = (\n",
    "    \"Compare document A (predictions) with document B (earnings report) \"\n",
    "    \"and classify predictions as confirmed/contradicted/undiscussed, plus \"\n",
    "    \"list new drivers.\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== FINAL ANSWER (quiet run) ===\\n\")\n",
    "print(compare_documents(prompt))\n",
    "\n",
    "# Verbose streaming --------------------------------------------------\n",
    "print(\"\\n=== STREAMED STEPS ===\\n\")\n",
    "for st in compare_documents_stream(prompt):\n",
    "    print(f\"step {st.steps:02d}\", st.trace[-1] if st.trace else \"<planning>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
